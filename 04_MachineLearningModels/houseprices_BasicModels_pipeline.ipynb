{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic ML Models of the Kaggle House Price Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "# numpy: support for large, multi-dimensional arrays and matrices and high-level mathematical functions\n",
    "import numpy as np\n",
    "print(\"numpy version: {}\". format(np.__version__))\n",
    "\n",
    "import sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso, Ridge, BayesianRidge, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"sklearn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "import optuna\n",
    "print(\"optuna version: {}\". format(optuna.__version__))\n",
    "\n",
    "import mlflow\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_PARENT_RUN_ID\n",
    "from mlflow.tracking import MlflowClient\n",
    "print(\"mlflow version: {}\". format(mlflow.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import yaml\n",
    "with open('ml_parameter.yaml') as file:\n",
    "  config_data= yaml.safe_load(file)\n",
    "\n",
    "\n",
    "from create_algorithm import create_algorithm\n",
    "import pipeline_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"1\"\n",
    "SCRIPT = \"test\"\n",
    "\n",
    "# VERSION = 0.6\n",
    "# SCRIPT = \"houseprices_BasicModels_pipeline\"\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient()\n",
    "try:\n",
    "    experiment = client.create_experiment(f\"{SCRIPT}_{VERSION}\")\n",
    "except:\n",
    "    experiment = client.get_experiment_by_name(f\"{SCRIPT}_{VERSION}\").experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StratifiedKFold_Regression(df, target, bins, n_splits):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    df['Fold'] = -1\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    y_cat = pd.cut(df[target], bins, labels=False)\n",
    "\n",
    "    for fold_no, (t, v) in enumerate(skf.split(y_cat, y_cat)):\n",
    "        df.loc[v, 'Fold'] = fold_no\n",
    "\n",
    "    sns.histplot(data=df, x=y_cat, hue=\"Fold\", multiple=\"stack\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_season(month, hemisphere):\n",
    "    if hemisphere == 'Southern':\n",
    "        season_month_south = {\n",
    "            12:'Summer', 1:'Summer', 2:'Summer',\n",
    "            3:'Autumn', 4:'Autumn', 5:'Autumn',\n",
    "            6:'Winter', 7:'Winter', 8:'Winter',\n",
    "            9:'Spring', 10:'Spring', 11:'Spring'}\n",
    "        return season_month_south.get(month)\n",
    "        \n",
    "    elif hemisphere == 'Northern':\n",
    "        season_month_north = {\n",
    "            12:'Winter', 1:'Winter', 2:'Winter',\n",
    "            3:'Spring', 4:'Spring', 5:'Spring',\n",
    "            6:'Summer', 7:'Summer', 8:'Summer',\n",
    "            9:'Autumn', 10:'Autumn', 11:'Autumn'}\n",
    "        return season_month_north.get(month)\n",
    "    else:\n",
    "        print('Invalid selection. Please select a hemisphere and try again')\n",
    "\n",
    "\n",
    "class CreateNewFeatures(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def fit(self, x:pd.DataFrame, y:Optional[pd.DataFrame]=None) -> \"CreateNewFeatures\":\n",
    "        return self\n",
    "\n",
    "    def transform(self, x:pd.DataFrame) -> pd.DataFrame:\n",
    "        print(x.shape)\n",
    "        x[\"TotalFS\"] = x[\"1stFlrSF\"] + x[\"2ndFlrSF\"] + x[\"GrLivArea\"]\n",
    "        x[\"MeanSFRoom\"] = round(x[\"TotalFS\"] / x[\"TotRmsAbvGrd\"], 0)\n",
    "        x[\"YearsBeforeWork\"] = x[\"YearRemodAdd\"] - x[\"YearBuilt\"]\n",
    "        x[\"TotalBath\"] = x[\"FullBath\"] + 0.5*x[\"HalfBath\"] + x[\"BsmtFullBath\"] + 0.5*x[\"BsmtHalfBath\"]\n",
    "        x[\"TotalFS_TotalBath\"] = x[\"TotalFS\"] / x[\"TotalBath\"]\n",
    "        x[\"GarageArea_GarageCars\"] = x[\"GarageArea\"] / x[\"GarageCars\"]\n",
    "        x[\"TotalPorchSF\"] = x[\"OpenPorchSF\"] + x[\"EnclosedPorch\"] + x[\"3SsnPorch\"] + x[\"ScreenPorch\"]\n",
    "        x[\"YearsBeforeSold\"] = x[\"YrSold\"] - x[\"YearBuilt\"]\n",
    "        x[\"SeasonSold\"] = x[\"MoSold\"].apply(lambda x: find_season(x, \"Northern\"))\n",
    "\n",
    "        x['PoolArea_bin'] = x['PoolArea'].apply(lambda x: 1 if x>0 else 0)\n",
    "        x['TotalPorchSF_bin'] = x['TotalPorchSF'].apply(lambda x: 1 if x>0 else 0)\n",
    "        x['GarageArea_bin'] = x['GarageArea'].apply(lambda x: 1 if x>0 else 0)\n",
    "        x['MiscVal_bin'] = x['MiscVal'].apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "        # transform month to continous feature\n",
    "        x['MoSold'] = (-np.cos(0.5236*x['MoSold']))\n",
    "\n",
    "        x.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # load raw training and test dataset\n",
    "    df_train = pd.read_csv(\"../01_RawData/train.csv\")\n",
    "    df_test = pd.read_csv(\"../01_RawData/test.csv\")\n",
    "\n",
    "    # drop features with more then 30% of missing values\n",
    "    df_train = df_train.drop([\"Id\", \"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\"], axis=1)\n",
    "    df_test = df_test.drop([\"Id\", \"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\"], axis=1)\n",
    "\n",
    "    feature_transformer = CreateNewFeatures()\n",
    "    feature_transformer.fit(df_train)\n",
    "    df_train = feature_transformer.transform(df_train)\n",
    "    df_test = feature_transformer.transform(df_test)\n",
    "\n",
    "    # compute StratifiedKFold column\n",
    "    df_train = StratifiedKFold_Regression(df_train, \"SalePrice\", 50, 5)\n",
    "\n",
    "    # split the training and test dataset to the input features (x_train, x_test) and the survival class (y_train)\n",
    "    y_train = df_train['SalePrice']\n",
    "    x_train = df_train.drop(['SalePrice'], axis=1)\n",
    "    x_test = df_test\n",
    "\n",
    "    x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    return x_train, y_train, x_validate, y_validate, x_test\n",
    "\n",
    "x_train, y_train, x_validate, y_validate, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_classes(x_train):\n",
    "    # numeric features\n",
    "    numeric_features = x_train.select_dtypes([np.number]).columns.to_list()\n",
    "    numeric_features.remove(\"Fold\")\n",
    "\n",
    "    # ordinal_features\n",
    "    ordinal_features = [\n",
    "        \"Utilities\",\n",
    "        \"LandSlope\",\n",
    "        \"HouseStyle\",\n",
    "        \"ExterQual\",\n",
    "        \"ExterCond\",\n",
    "        \"BsmtQual\",\n",
    "        \"BsmtCond\",\n",
    "        \"GarageQual\",\n",
    "        \"GarageCond\",\n",
    "        \"BsmtExposure\",\n",
    "        \"BsmtFinType1\",\n",
    "        \"BsmtFinType2\",\n",
    "        \"HeatingQC\",\n",
    "        \"KitchenQual\",\n",
    "        \"Functional\",\n",
    "        \"GarageFinish\",\n",
    "        \"PavedDrive\",\n",
    "        \"SeasonSold\"\n",
    "        ]\n",
    "    \n",
    "    category_features = x_train.select_dtypes(['object', 'category'])\n",
    "\n",
    "    # categorical features with high cardinality\n",
    "    category_features_high_cardinality = category_features.columns[category_features.nunique() >50].tolist()\n",
    "    category_features_high_cardinality = list(set(category_features_high_cardinality) - set(ordinal_features))\n",
    "    \n",
    "    # categorical features with low cardinality\n",
    "    category_features_low_cardinality = category_features.columns[category_features.nunique() <=50].tolist()\n",
    "    category_features_low_cardinality = list(set(category_features_low_cardinality) - set(ordinal_features))\n",
    "\n",
    "    return numeric_features, ordinal_features, category_features_high_cardinality, category_features_low_cardinality\n",
    "\n",
    "numeric_features, ordinal_features, category_features_high_cardinality, category_features_low_cardinality = get_feature_classes(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cross validation\n",
    "\n",
    "# cv = ShuffleSplit(\n",
    "#     n_splits = 10,\n",
    "#     test_size = 0.2,\n",
    "#     random_state = 42\n",
    "#     )\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "cv = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(self, trial):\n",
    "\n",
    "    child_run = client.create_run(\n",
    "        experiment_id=experiment,\n",
    "        tags={\n",
    "            MLFLOW_PARENT_RUN_ID: self.parent_run.info.run_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = create_algorithm(self.model_type, trial, client, child_run, config_data)\n",
    "\n",
    "    # create parameter for data preprocessing pipeline\n",
    "    imputer_function_numeric=trial.suggest_categorical(\"preprocessing_imputer_function_numeric\", [\"SimpleImputer\", \"KNNImputer\"])\n",
    "    skewed_threshold=trial.suggest_float(\"preprocessing_skewed_threshold\", 0.5, 2, log=False)\n",
    "    ohe_min_frequency=trial.suggest_float(\"preprocessing_ohe_min_frequency\", 0, 0.2, log=False)\n",
    "    ohe_max_categories=trial.suggest_int('preprocessing_ohe_max_categories', 5, 20)\n",
    "    feature_selection_low_variance_flag=trial.suggest_float(\"preprocessing_feature_selection_low_variance_flag\", 0.7, 0.99, log=False)\n",
    "    correlation=trial.suggest_float(\"preprocessing_correlation\", 0.7, 0.99, log=False)\n",
    "    scaler = trial.suggest_categorical(\"columnprep__transformers_num\", [\"StandardScaler\", \"MinMaxScaler\"])\n",
    "    pca_n_components = trial.suggest_float('pca_n_components', 0, 1)\n",
    "\n",
    "    # log all parameters of the data preprocessing with mlflow\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_imputer_function_numeric\", imputer_function_numeric)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_skewed_threshold\", skewed_threshold)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_ohe_min_frequency\", ohe_min_frequency)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_ohe_max_categories\", ohe_max_categories)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_feature_selection_low_variance_flag\", feature_selection_low_variance_flag)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_correlation\", correlation)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_scaler\", scaler)  \n",
    "    client.log_param(child_run.info.run_id, \"pca_n_components\", pca_n_components)\n",
    "\n",
    "\n",
    "    # build numeric part of the pipeline\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", pipeline_classes.ImputMissingValuesNumeric(imputer_function=imputer_function_numeric)),\n",
    "        (\"skew\", pipeline_classes.SkewedFeatureTransformer(skewed_threshold=skewed_threshold)),\n",
    "        (\"scaler\", pipeline_classes.ScalerTransformer(columnprep__transformers_num=scaler))\n",
    "        ])\n",
    "\n",
    "    # build categoric part of the pipeline\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy='most_frequent', missing_values=np.nan)),\n",
    "            (\"encoder\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\", drop=\"if_binary\"))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # build ordinal part of the pipeline\n",
    "    ordinal_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy='most_frequent', missing_values=np.nan)),\n",
    "            (\"encoder\", OrdinalEncoder(categories=[\n",
    "                [\"ELO\", \"NoSeWa\", \"NoSewr\", \"AllPub\"],\n",
    "                [\"Gtl\", \"Mod\", \"Sev\"],\n",
    "                [\"1Story\", \"1.5Fin\", \"1.5Unf\", \"2Story\", \"2.5Fin\", \"2.5Unf\", \"SFoyer\", \"SLvl\"],\n",
    "                [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "                [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "                [\"NA\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "                [\"NA\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "                [\"NA\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "                [\"NA\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "                [\"NA\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "                [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "                [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "                [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "                [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "                [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "                [\"NA\", \"Unf\", \"RFn\", \"Fin\"],\n",
    "                [\"Y\", \"P\", \"N\"],\n",
    "                [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]\n",
    "                ])\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, category_features_low_cardinality),\n",
    "            (\"ord\", ordinal_transformer, ordinal_features)\n",
    "        ], remainder='drop'\n",
    "    )\n",
    "\n",
    "    feature_selection = Pipeline(steps=[\n",
    "        (\"variance\", VarianceThreshold()),\n",
    "        (\"correlation\", pipeline_classes.CorrelationTransformer(correlation_threshold=correlation)),\n",
    "        (\"PCA\", PCA(n_components=pca_n_components)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_selection\", feature_selection),\n",
    "        (\"regressor\", model)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline, child_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(best_model, x_test, parent_run, study):\n",
    "    # check if folder exists\n",
    "    if not os.path.exists(f'submissions/{SCRIPT}/{VERSION}'):\n",
    "        os.makedirs(f'submissions/{SCRIPT}/{VERSION}')\n",
    "    \n",
    "    # predict the test values with the training classification model\n",
    "    y_pred = best_model.predict(x_test)\n",
    "    \n",
    "    df_submission = pd.read_csv(\"../01_RawData/sample_submission.csv\")\n",
    "    df_submission.iloc[:, 1] = y_pred\n",
    "    \n",
    "    df_submission.to_csv(f'submissions/{SCRIPT}/{VERSION}/{parent_run.info.run_id}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_train, y_train, y_validate, y_validate_pred, pipeline, parent_run):\n",
    "\n",
    "    def plot_learning_curve(pipeline, x_train, y_train, cv=cv, train_sizes=np.linspace(.1, 1.0, 50)):\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            pipeline,\n",
    "            x_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            groups=x_train[\"Fold\"],\n",
    "            train_sizes=np.linspace(.1, 1.0, 8)\n",
    "            )\n",
    "\n",
    "\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel(\"Training examples\")\n",
    "        ax1.set_ylabel(\"Score\")\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        ax1.grid()\n",
    "\n",
    "        ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                        train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                        color=\"r\")\n",
    "        ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                        test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "        ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                label=\"Training score\")\n",
    "        ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                label=\"Cross-validation score\")\n",
    "\n",
    "        ax1.legend(loc=\"best\")\n",
    "        ax1.set_title(\"Difference between training and CV: \"\\\n",
    "            + str(round(test_scores_mean[7] / train_scores_mean[7] * 100, 2))\\\n",
    "            + \"%\")\n",
    "        client.log_figure(parent_run.info.run_id, fig1, 'plot_learning_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "    plot_learning_curve(pipeline, x_train, y_train)\n",
    "\n",
    "\n",
    "    def plot_scatter(y_true, y_pred):\n",
    "        fig2, ax2 = plt.subplots()\n",
    "        ax2.scatter(y_pred, y_true.values)\n",
    "        ax2.plot([min(y_pred), max(y_pred)], [min(y_pred), max(y_pred)], c=\"red\")\n",
    "        ax2.set_ylabel(\"True Values\")\n",
    "        ax2.set_xlabel(\"Predicted Values\")\n",
    "        client.log_figure(parent_run.info.run_id, fig2, 'plot_regression.png')\n",
    "        plt.close()\n",
    "\n",
    "    plot_scatter(y_validate, y_validate_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    \n",
    "    def __init__(self, model_type, parent_run):\n",
    "        self.best_model = None\n",
    "        self._model = None\n",
    "        self.model_type = model_type\n",
    "        self.parent_run = parent_run\n",
    "\n",
    "    \n",
    "    def __call__(self, trial):\n",
    "    \n",
    "        pipeline, child_run = create_model(self, trial)\n",
    "        self._model = pipeline\n",
    "\n",
    "        cvs = np.sqrt(\n",
    "            -cross_val_score(\n",
    "                pipeline,\n",
    "                x_train.drop(columns=[\"Fold\"]), y_train,\n",
    "                cv=cv,\n",
    "                groups=x_train[\"Fold\"],\n",
    "                scoring=\"neg_mean_squared_error\",\n",
    "                n_jobs=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "        rmse = cvs.mean()\n",
    "        rmse_var = np.var(cvs)\n",
    "\n",
    "        client.log_metric(child_run.info.run_id, \"cv_rmse\", rmse)\n",
    "        client.log_metric(child_run.info.run_id, \"cv_var\", rmse_var)\n",
    "        \n",
    "        return rmse\n",
    "\n",
    "    def callback(self, study, trial):\n",
    "        if study.best_trial == trial:\n",
    "            self.best_model = self._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type='Lasso'\n",
    "model_type='Ridge'\n",
    "# model_type='BayesianRidge'\n",
    "# model_type='ElasticNet'\n",
    "# model_type='GradientBoostingRegressor'\n",
    "# model_type='RandomForestRegressor'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parent_run = client.create_run(\n",
    "    experiment_id=experiment, \n",
    "    tags={\"script_name\": SCRIPT, \"script_version\": VERSION}\n",
    "    )\n",
    "\n",
    "objective = Objective(model_type, parent_run)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=config_data[\"N_TRAILS\"],\n",
    "    timeout=config_data[\"TIMEOUT\"],\n",
    "    callbacks=[objective.callback]\n",
    "    )\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_value)\n",
    "print(study.best_params)\n",
    "\n",
    "print(\"Log CV Params\")\n",
    "client.log_metric(parent_run.info.run_id, \"best_cv_score\", round(study.best_value, 3))\n",
    "# client.log_param(parent_run.info.run_id, \"transformer_num\", str(transformer_num))\n",
    "\n",
    "# client.log_param(parent_run.info.run_id, \"cv_n_splits\", cv.n_splits)\n",
    "# client.log_param(parent_run.info.run_id, \"cv_train_size\", cv.train_size)\n",
    "# client.log_param(parent_run.info.run_id, \"cv_test_size\", cv.test_size)\n",
    "# client.log_param(parent_run.info.run_id, \"cv_random_state\", cv.random_state)\n",
    "\n",
    "for param in study.best_params:\n",
    "    client.log_param(parent_run.info.run_id, param, study.best_params[param])\n",
    "\n",
    "best_model = objective.best_model\n",
    "client.log_param(parent_run.info.run_id, \"algo\", best_model.get_params()[\"steps\"][-1][1].__class__.__name__)\n",
    "\n",
    "\n",
    "# fit the pipeline to compute the validation results\n",
    "print(\"Fit Best Model\")\n",
    "best_model.fit(x_train.drop(columns=[\"Fold\"]), y_train)\n",
    "\n",
    "# predict the training outcome\n",
    "print(\"Predict Best Model\")\n",
    "y_validate_pred = best_model.predict(x_validate)\n",
    "\n",
    "# evaluate model\n",
    "print(\"Evaluate Best Model\")\n",
    "client.log_metric(parent_run.info.run_id, \"r2\", r2_score(y_validate, y_validate_pred))\n",
    "evaluate_model(x_train, y_train, y_validate, y_validate_pred, best_model, parent_run)\n",
    "\n",
    "# # create submission of best model\n",
    "print(\"Create submission\")\n",
    "create_submission(best_model, x_test, parent_run, study)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_optuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4aa63c7bcea9117a32328ad03333d01dc516bdcdb33b6eb92ab7a393341400f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
